# BegoniaGPT
Large language models (LLMs) have taken the natural language processing (NLP) domain by storm, and their transformative momentum has surged into the domain of education, giving rise to a nascent wave of education-tailored LLMs.

Despite their potential to facilitate homework assistance, such LLMs fall short in the fine-grained domain of elementary and secondary school (i.e., K-12) education. They often indiscriminately incorporate broad knowledge across diverse disciplines, overlooking the stark disparities in cognitive demands and curricular content among elementary, middle, and high school phases. This one-size-fits-all approach fails to cater to the unique learning needs of primary and secondary education, leading to the misalignment between current LLMs and the real-world scenarios.

To fill this gap, we propose a new English teaching LLM, called BegoniaGPT, which discards irrelevant knowledge from other disciplines, and shapes the general LLM to be  an exceptional English teacher by emphasizing four key aspects: foundational English knowledge, professional proficiency, international vision, and psychological support. In particular, we build a large-scale English corpus named EngCorpus. 

By continued pre-training and supervised fine-tuning the general LLM on the carefully curated EngCorpus and aligning it with reinforcement learning with expert feedback, BegoniaGPT could provide refined, specialized, personalized and compassionate English education. 
